{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backpropagation from Scratch Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y871RZtRhCEH"
      },
      "source": [
        "# <font color='red'>Backpropagation</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqePYi-4mFvK"
      },
      "source": [
        "\n",
        "In this assignment, you will implement Backpropagation from scratch. You will then verify the correctness of the your implementation using a \"grader\" function/cell (provided by us) which will match your implmentation.\n",
        "\n",
        "The grader fucntion would help you validate the correctness of your code. \n",
        "\n",
        "Please submit the final Colab notebook in the classroom ONLY after you have verified your code using the grader function/cell.\n",
        "\n",
        "\n",
        "**NOTE: DO NOT change the \"grader\" functions or code snippets written by us.Please add your code in the suggested locations.**\n",
        "\n",
        "Ethics Code:\n",
        "1. You are welcome to read up online resources to implement the code. \n",
        "2. You can also discuss with your classmates on the implmentation over Slack.\n",
        "3. But, the code you write and submit should be yours ONLY. Your code will be compared against other stduents' code and online code snippets to check for plagiarism. If your code is found to be plagiarised, you will be awarded zero-marks for all assignments, which have a 10% weightage in the final marks for this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0huUQ0byiI0I"
      },
      "source": [
        "## <font color='red'>Loading data </font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "M3JIziCsVtdL",
        "outputId": "30e9f90f-d375-4b27-a146-99ff0e69ecff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 135\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hSje5CBgcUb"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "with open('/content/drive/MyDrive/AppliedRootsAIML/Semester2/Backpropagation_from_Scratch_Assignment_Data.pkl','rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(data.shape)\n",
        "X = data[:, :5]\n",
        "y = data[:, -1]\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "id": "XHAKhIO6Yaim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmphIg_E52Rs"
      },
      "source": [
        "<font color='blue'><b>Check this video for better understanding of the computational graphs and back propagation</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUWpHhgg53eu"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('i94OvYb6noo',width=\"1000\",height=\"500\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JL-0soQistC"
      },
      "source": [
        "# <font color='red'>Computational graph</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nREnTTJ3i0Vd"
      },
      "source": [
        "<img src='https://i.imgur.com/seSGbNS.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSPX_H_4i_HT"
      },
      "source": [
        "\n",
        "*  **If you observe the graph, we are having input features [f1, f2, f3, f4, f5] and 9 weights [w1, w2, w3, w4, w5, w6,    w7, w8, w9]**.<br><br>\n",
        "*  **The final output of this graph is a value L which is computed as (Y-Y')^2** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D54eDEv6jkO4"
      },
      "source": [
        "## <font color='red'>Task 1: Implementing Forward propagation, Backpropagation and Gradient checking </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyWK94o_fTKH"
      },
      "source": [
        "# <font color='red'>Task 1.1 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwEcPWLffTKI"
      },
      "source": [
        "## <font color='blue'>Forward propagation </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCZrm-gkfTKI"
      },
      "source": [
        "\n",
        "*  <b>\n",
        "    Forward propagation</b>(Write your code in<font color='blue'> def forward_propagation()</b></font>)<br><br>\n",
        "    For easy debugging, we will break the computational graph into 3 parts.\n",
        "\n",
        "    <font color='green'><b>Part 1</b></font></b>\n",
        "    <img src='https://i.imgur.com/0xUaxy6.png'><br><br>\n",
        "    <font color='green'><b>Part 2</b></font></b><br>\n",
        "    <img src='https://i.imgur.com/J29pAJL.png'><br><br>\n",
        "    <font color='green'><b>Part 3</b></font></b>\n",
        "    <img src='https://i.imgur.com/vMyCsd9.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADCovl2FfTKJ"
      },
      "source": [
        "def sigmoid(z):\n",
        "    '''In this function, we will compute the sigmoid(z)'''\n",
        "    # we can use this function in forward and backward propagation\n",
        "    # write the code to compute the sigmoid value of z and return that value \n",
        "    return 1/(1+np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLJ-OYwefTKJ"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  #if you have written the code correctly then the grader function will output true\n",
        "  val=sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "grader_sigmoid(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD3piNkifTKJ"
      },
      "source": [
        "def forward_propagation(x, y, w):\n",
        "        '''In this function, we will compute the forward propagation '''\n",
        "        # X: input data point, note that in this assignment you are having 5-d data points\n",
        "        # y: output varible\n",
        "        # W: weight array, its of length 9, W[0] corresponds to w1 in graph, W[1] corresponds to w2 in graph,..., W[8] corresponds to w9 in graph.  \n",
        "        # you have to return the following variables\n",
        "        # exp= part1 (compute the forward propagation until exp and then store the values in exp)\n",
        "        # tanh =part2(compute the forward propagation until tanh and then store the values in tanh)\n",
        "        # sig = part3(compute the forward propagation until sigmoid and then store the values in sig)\n",
        "        # we are computing one of the values for better understanding\n",
        "        \n",
        "        val_1= (w[0]*x[0]+w[1]*x[1]) * (w[0]*x[0]+w[1]*x[1]) + w[5]\n",
        "        part_1 = np.exp(val_1)\n",
        "        val_2= part_1 + w[6]\n",
        "        part_2 = np.tanh(val_2)\n",
        "        val_3 = (np.sin(w[2]*x[2]) * ((w[3] * x[3]) + (w[4] * x[4]))) + w[7]\n",
        "        part_3= sigmoid(val_3)\n",
        "        \n",
        "        # after computing part1,part2 and part3 compute the value of y' from the main Computational graph using required equations\n",
        "        y_pred = part_2 + (part_3 * w[8])        \n",
        "        # write code to compute the value of L=(y-y')^2 and store it in variable loss\n",
        "        L = (y - y_pred)**2\n",
        "        # compute derivative of L  w.r.to y' and store it in dy_pred \n",
        "        dy_pred = 2 * (y - y_pred) * (-1)\n",
        "        # Create a dictionary to store all the intermediate values i.e. dy_pred ,loss,exp,tanh,sigmoid\n",
        "        # we will be using the dictionary to find values in backpropagation, you can add other keys in dictionary as well\n",
        "        \n",
        "        forward_dict={}\n",
        "        forward_dict['exp']= part_1\n",
        "        forward_dict['sigmoid'] = part_3\n",
        "        forward_dict['tanh'] = part_2\n",
        "        forward_dict['loss'] = L\n",
        "        forward_dict['dy_pred'] = dy_pred\n",
        "        \n",
        "        return forward_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP0iV7f4fTKK"
      },
      "source": [
        "def grader_forwardprop(data):\n",
        "    dl = (data['dy_pred']==-1.9285278284819143)\n",
        "    loss=(data['loss']==0.9298048963072919)\n",
        "    part1=(data['exp']==1.1272967040973583)\n",
        "    part2=(data['tanh']==0.8417934192562146)\n",
        "    part3=(data['sigmoid']==0.5279179387419721)\n",
        "    assert(dl and loss and part1 and part2 and part3)\n",
        "    return True\n",
        "w=np.ones(9)*0.1\n",
        "d1=forward_propagation(X[0],y[0],w)\n",
        "grader_forwardprop(d1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zcQmvkvfTKL"
      },
      "source": [
        "# <font color='red'>Task 1.2 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YVYD18xfTKL"
      },
      "source": [
        "## <font color='blue'>Backward propagation</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld1PMQtffTKL"
      },
      "source": [
        "# def backward_propagation(x,y,w,forward_dict):\n",
        "#     '''In this function, we will compute the backward propagation '''\n",
        "#     # forward_dict: the outputs of the forward_propagation() function\n",
        "#     # write code to compute the gradients of each weight [w1,w2,w3,...,w9]\n",
        "#     # Hint: you can use dict type to store the required variables \n",
        "#     # dw1 = # in dw1 compute derivative of L w.r.to w1\n",
        "#     # dw2 = # in dw2 compute derivative of L w.r.to w2\n",
        "#     # dw3 = # in dw3 compute derivative of L w.r.to w3\n",
        "#     # dw4 = # in dw4 compute derivative of L w.r.to w4\n",
        "#     # dw5 = # in dw5 compute derivative of L w.r.to w5\n",
        "#     # dw6 = # in dw6 compute derivative of L w.r.to w6\n",
        "#     # dw7 = # in dw7 compute derivative of L w.r.to w7\n",
        "#     # dw8 = # in dw8 compute derivative of L w.r.to w8\n",
        "#     # dw9 = # in dw9 compute derivative of L w.r.to w9\n",
        "#     dw9 = forward_dict['dy_pred'] * 1 * forward_dict['sigmoid']\n",
        "#     dO3O4 = forward_dict['sigmoid'] * (1- forward_dict['sigmoid'])\n",
        "#     dLO4 = forward_dict['dy_pred'] * 1 * 0.1 * dO3O4\n",
        "#     dw8 = dLO4 * 1\n",
        "#     dLO6 = forward_dict['dy_pred'] * 1 * (1 - (forward_dict['tanh']**2))\n",
        "#     dw7 = 1 * dLO6\n",
        "#     dLO12 = forward_dict['exp'] * 1 * dLO6\n",
        "#     dw6 = 1 * dLO12\n",
        "#     dLO16 = 1 * (-0.002546) * 1 * dLO4\n",
        "#     dw5 = -0.14421743 * dLO16\n",
        "#     dLO15 = np.round(1 * (-0.002546) * 1 * dLO4,6)\n",
        "#     #dw4 = -0.66660821 * dLO15    \n",
        "#     dw4 = np.round(-0.666608 * dLO15,6)    \n",
        "#     O18 = (x[3] * w[3]) + (x[4] * w[4])\n",
        "#     dLO17 = O18 * 1 * dLO4    \n",
        "#     dLO14 = math.cos(x[2] * w[2]) * dLO17\n",
        "#     #dLO14 = 0.999997 * dLO17\n",
        "#     dw3 = x[2] * dLO14\n",
        "#     #dw3 = -1.45900038 * dLO14\n",
        "#     dLO8 = (1 * (-0.140792) * 1 * dLO12) + (1 * (-0.140792) * 1 * dLO12)\n",
        "#     dw2 = -0.12001342 * dLO8\n",
        "#     dLO7 = dLO8\n",
        "#     dw1 = -1.2879095 * dLO7\n",
        "#     backward_dict={}\n",
        "#     #store the variables dw1,dw2 etc. in a dict as backward_dict['dw1']= dw1,backward_dict['dw2']= dw2...\n",
        "#     backward_dict['dw1']= dw1\n",
        "#     backward_dict['dw2']= dw2\n",
        "#     backward_dict['dw3']= dw3\n",
        "#     backward_dict['dw4']= dw4\n",
        "#     backward_dict['dw5']= dw5\n",
        "#     backward_dict['dw6']= dw6\n",
        "#     backward_dict['dw7']= dw7\n",
        "#     backward_dict['dw8']= dw8\n",
        "#     backward_dict['dw9']= dw9\n",
        "#     #print(dw1 + \",\"+ dw2+ \",\"+dw3+ \",\"+dw4+ \",\"+dw5+ \",\"+dw6+ \",\"+dw7+ \",\"+dw8+ \",\"+dw9)\n",
        "#     print(dw1)\n",
        "#     print(dw2)\n",
        "#     print(np.round(dw3,6))\n",
        "#     print(dw4)\n",
        "#     print(dw5)\n",
        "#     print(dw6)\n",
        "#     print(dw7)\n",
        "#     print(dw8)\n",
        "#     print(dw9)\n",
        "#     print(O18)\n",
        "#     return backward_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TCzo75BuwnMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCJBYefRwnv6"
      },
      "source": [
        "def backward_propagation(x,y,w,forward_dict):\n",
        "    '''In this function, we will compute the backward propagation '''\n",
        "    # forward_dict: the outputs of the forward_propagation() function\n",
        "    # write code to compute the gradients of each weight [w1,w2,w3,...,w9]\n",
        "    # Hint: you can use dict type to store the required variables \n",
        "    # dw1 = # in dw1 compute derivative of L w.r.to w1\n",
        "    # dw2 = # in dw2 compute derivative of L w.r.to w2\n",
        "    # dw3 = # in dw3 compute derivative of L w.r.to w3\n",
        "    # dw4 = # in dw4 compute derivative of L w.r.to w4\n",
        "    # dw5 = # in dw5 compute derivative of L w.r.to w5\n",
        "    # dw6 = # in dw6 compute derivative of L w.r.to w6\n",
        "    # dw7 = # in dw7 compute derivative of L w.r.to w7\n",
        "    # dw8 = # in dw8 compute derivative of L w.r.to w8\n",
        "    # dw9 = # in dw9 compute derivative of L w.r.to w9\n",
        "    dw9 = forward_dict['dy_pred'] * 1 * forward_dict['sigmoid']\n",
        "    dO3O4 = forward_dict['sigmoid'] * (1- forward_dict['sigmoid'])\n",
        "    dLO4 = forward_dict['dy_pred'] * 1 * w[8] * dO3O4\n",
        "    dw8 = dLO4 * 1\n",
        "    dLO6 = forward_dict['dy_pred'] * 1 * (1 - (forward_dict['tanh']**2))\n",
        "    dw7 = 1 * dLO6\n",
        "    dLO12 = forward_dict['exp'] * 1 * dLO6\n",
        "    dw6 = 1 * dLO12\n",
        "    O17 = math.sin(w[2] * x[2])\n",
        "    dLO16 = 1 * O17 * 1 * dLO4\n",
        "    dw5 = x[4] * dLO16\n",
        "    dLO15 = 1 * O17 * 1 * dLO4\n",
        "    dw4 = x[3] * dLO15\n",
        "    O18 = (x[3] * w[3]) + (x[4] * w[4])\n",
        "    dLO17 = O18 * 1 * dLO4    \n",
        "    dLO14 = math.cos(x[2] * w[2]) * dLO17\n",
        "    dw3 = x[2] * dLO14\n",
        "    #dLO8 = (1 * (-0.140792) * 1 * dLO12) + (1 * (-0.140792) * 1 * dLO12)\n",
        "    O9 = (w[0] * x[0]) + (w[1] * x[1])\n",
        "    O10 = (w[0] * x[0]) + (w[1] * x[1])\n",
        "    dLO8 = (1 * O9 * 1 * dLO12) + (1 * O10 * 1 * dLO12)\n",
        "    dw2 = x[1] * dLO8\n",
        "    dLO7 = dLO8\n",
        "    dw1 = x[0] * dLO7\n",
        "    backward_dict={}\n",
        "    #store the variables dw1,dw2 etc. in a dict as backward_dict['dw1']= dw1,backward_dict['dw2']= dw2...\n",
        "    backward_dict['dw1']= dw1\n",
        "    backward_dict['dw2']= dw2\n",
        "    backward_dict['dw3']= dw3\n",
        "    backward_dict['dw4']= dw4\n",
        "    backward_dict['dw5']= dw5\n",
        "    backward_dict['dw6']= dw6\n",
        "    backward_dict['dw7']= dw7\n",
        "    backward_dict['dw8']= dw8\n",
        "    backward_dict['dw9']= dw9\n",
        "    #print(dw1 + \",\"+ dw2+ \",\"+dw3+ \",\"+dw4+ \",\"+dw5+ \",\"+dw6+ \",\"+dw7+ \",\"+dw8+ \",\"+dw9)\n",
        "    # print(dw1)\n",
        "    # print(dw2)\n",
        "    # print(dw3)\n",
        "    # print(dw4)\n",
        "    # print(dw5)\n",
        "    # print(dw6)\n",
        "    # print(dw7)\n",
        "    # print(dw8)\n",
        "    # print(dw9)\n",
        "    # print(O18)\n",
        "    return backward_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ipQbNXOfTKM"
      },
      "source": [
        "def grader_backprop(data):\n",
        "    dw1=(np.round(data['dw1'],6)==-0.229733)\n",
        "    dw2=(np.round(data['dw2'],6)==-0.021408)\n",
        "    dw3=(np.round(data['dw3'],6)==-0.005625)\n",
        "    dw4=(np.round(data['dw4'],6)==-0.004658)\n",
        "    dw5=(np.round(data['dw5'],6)==-0.001008)\n",
        "    dw6=(np.round(data['dw6'],6)==-0.633475)\n",
        "    dw7=(np.round(data['dw7'],6)==-0.561942)\n",
        "    dw8=(np.round(data['dw8'],6)==-0.048063)\n",
        "    dw9=(np.round(data['dw9'],6)==-1.018104)\n",
        "    assert(dw1 and dw2 and dw3 and dw4 and dw5 and dw6 and dw7 and dw8 and dw9)\n",
        "    return True \n",
        "w=np.ones(9)*0.1\n",
        "forward_dict=forward_propagation(X[0],y[0],w)\n",
        "backward_dict=backward_propagation(X[0],y[0],w,forward_dict)\n",
        "grader_backprop(backward_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlR1JVN5fTKM"
      },
      "source": [
        "# <font color='red'>Task 1.3 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STS4NrQQc6OH"
      },
      "source": [
        " ## <font color='blue'>Gradient clipping</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY7ivRNzdPok"
      },
      "source": [
        "<b> Check this  <a href='https://towardsdatascience.com/how-to-debug-a-neural-network-with-gradient-checking-41deec0357a9'>blog link</a> for more details on Gradient clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrsfpDoidtZ5"
      },
      "source": [
        " we know that the derivative of any function is\n",
        " \n",
        " $$\\lim_{\\epsilon\\to0}\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUcmt0kPd02f"
      },
      "source": [
        "*  The definition above can be used as a numerical approximation of the derivative. Taking an epsilon small enough, the calculated approximation will have an error in the range of epsilon squared. \n",
        "\n",
        "*  In other words, if epsilon is 0.001, the approximation will be off by 0.00001.\n",
        "\n",
        "Therefore, we can use this to approximate the gradient, and in turn make sure that backpropagation is implemented properly. This forms the basis of <b>gradient checking!</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFSu16KCeU0x"
      },
      "source": [
        "## <Font color='blue'>Gradient checking example</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz0mmT_xecfC"
      },
      "source": [
        "<font >\n",
        "lets understand the concept with a simple example:\n",
        "$f(w1,w2,x1,x2)=w_{1}^{2} . x_{1} + w_{2} . x_{2}$ \n",
        "\n",
        "from the above function , lets assume $w_{1}=1$, $w_{2}=2$, $x_{1}=3$, $x_{2}=4$ the gradient of $f$ w.r.t $w_{1}$ is\n",
        "\n",
        "\\begin{array} {lcl}\n",
        "\\frac{df}{dw_{1}} = dw_{1} &=&2.w_{1}.x_{1} \\\\& = &2.1.3\\\\& = &6\n",
        "\\end{array}\n",
        "\n",
        "\n",
        "let calculate the aproximate gradient of $w_{1}$ as mentinoned in the above formula and considering $\\epsilon=0.0001$\n",
        "\n",
        "\\begin{array} {lcl}\n",
        "dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((1+0.0001)^{2} . 3 + 2 . 4) - ((1-0.0001)^{2} . 3 + 2 . 4)}{2\\epsilon} \\\\ & = & \\frac{(1.00020001 . 3 + 2 . 4) - (0.99980001. 3 + 2 . 4)}{2*0.0001} \\\\ & = & \\frac{(11.00060003) - (10.99940003)}{0.0002}\\\\ & = & 5.99999999999\n",
        "\\end{array}\n",
        "\n",
        "Then, we apply the following formula for gradient check: <i>gradient_check</i> = \n",
        "$\\frac{\\left\\Vert\\left (dW-dW^{approx}\\rm\\right) \\right\\Vert_2}{\\left\\Vert\\left (dW\\rm\\right) \\right\\Vert_2+\\left\\Vert\\left (dW^{approx}\\rm\\right) \\right\\Vert_2}$\n",
        "\n",
        "The equation above is basically the Euclidean distance normalized by the sum of the norm of the vectors. We use normalization in case that one of the vectors is very small.\n",
        "As a value for epsilon, we usually opt for 1e-7. Therefore, if gradient check return a value less than 1e-7, then it means that backpropagation was implemented correctly. Otherwise, there is potentially a mistake in your implementation. If the value exceeds 1e-3, then you are sure that the code is not correct.\n",
        "\n",
        "in our example: <i>gradient_check</i> $ = \\frac{(6 - 5.999999999994898)}{(6 + 5.999999999994898)} = 4.2514140356330737e^{-13}$\n",
        "\n",
        "you can mathamatically derive the same thing like this\n",
        "\n",
        "\\begin{array} {lcl}\n",
        "dw_1^{approx} & = & \\frac{f(w1+\\epsilon,w2,x1,x2)-f(w1-\\epsilon,w2,x1,x2)}{2\\epsilon} \\\\ & = & \\frac{((w_{1}+\\epsilon)^{2} . x_{1} + w_{2} . x_{2}) - ((w_{1}-\\epsilon)^{2} . x_{1} + w_{2} . x_{2})}{2\\epsilon} \\\\ & = & \\frac{4. \\epsilon.w_{1}. x_{1}}{2\\epsilon} \\\\ & = &  2.w_{1}.x_{1}\n",
        "\\end{array}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1BFQQaCermK"
      },
      "source": [
        "## <font color='red'> Implement Gradient checking </font> <br>\n",
        " (Write your code in <font color='blue'> def gradient_checking()</font>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqpfA3AqfJba"
      },
      "source": [
        "**Algorithm** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL39KeRFfNoD"
      },
      "source": [
        "<pre>\n",
        "<font color='darkblue'>\n",
        "W = initilize_randomly\n",
        "def gradient_checking(data_point, W):<font color='grey'>\n",
        "    # compute the L value using forward_propagation()\n",
        "    # compute the gradients of W using backword_propagation()</font>\n",
        "    approx_gradients = []\n",
        "    for each wi weight value in W:<font color='grey'>\n",
        "        # add a small value to weight wi, and then find the values of L with the updated weights\n",
        "        # subtract a small value to weight wi, and then find the values of L with the updated weights\n",
        "        # compute the approximation gradients of weight wi</font>\n",
        "        approx_gradients.append(approximation gradients of weight wi)<font color='grey'>\n",
        "    # compare the gradient of weights W from backword_propagation() with the aproximation gradients of weights with <br>  gradient_check formula</font>\n",
        "    return gradient_check</font>\n",
        "<b>NOTE: you can do sanity check by checking all the return values of gradient_checking(),<br> they have to be zero. if not you have bug in your code\n",
        "</pre></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBoJRqAwfTKO"
      },
      "source": [
        "def gradient_checking(x,y,w,eps):\n",
        "    # compute the dict value using forward_propagation()\n",
        "    # compute the actual gradients of W using backword_propagation()\n",
        "    forward_dict=forward_propagation(x,y,w)\n",
        "    backward_dict=backward_propagation(x,y,w,forward_dict)\n",
        "    \n",
        "    #we are storing the original gradients for the given datapoints in a list\n",
        "    \n",
        "    original_gradients_list=list(backward_dict.values())\n",
        "    # make sure that the order is correct i.e. first element in the list corresponds to  dw1 ,second element is dw2 etc.\n",
        "    # you can use reverse function if the values are in reverse order\n",
        "    \n",
        "    #now we have to write code for approx gradients, here you have to make sure that you update only one weight at a time\n",
        "    approx_gradients_list=[]\n",
        "    for i in range(len(w)):\n",
        "        # add a small value to weight wi, and then find the values of loss_plus with the updated weights\n",
        "        w_plus=w.copy()\n",
        "        w_plus[i] = w_plus[i]+eps\n",
        "        forward_dict = forward_propagation(x,y,w_plus)\n",
        "        loss_plus   = forward_dict['loss']\n",
        "        \n",
        "        # subtract a small value to weight wi, and then find the values of loss_minus with the updated weights\n",
        "        # please write your code below to calcualte loss_minus\n",
        "        w_minus=w.copy()\n",
        "        w_minus[i] = w_minus[i]-eps\n",
        "        forward_dict = forward_propagation(x,y,w_minus)\n",
        "        loss_minus   = forward_dict['loss']\n",
        "        \n",
        "        # now we will calualte approximate gradient for a particular weight\n",
        "        approx_gradient = (loss_plus-loss_minus)/(2*eps)\n",
        "        approx_gradients_list.append(approx_gradient)\n",
        "    \n",
        "    #performing gradient check operation\n",
        "    original_gradients_list=np.array(original_gradients_list)\n",
        "    approx_gradients_list=np.array(approx_gradients_list)\n",
        "    gradient_check_value =(original_gradients_list-approx_gradients_list)/(original_gradients_list+approx_gradients_list)\n",
        "    \n",
        "    return gradient_check_value\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWSU56GffTKO"
      },
      "source": [
        "def grader_grad_check(value):\n",
        "    print(value)\n",
        "    assert(np.all(value < 10**-4))\n",
        "    return True \n",
        "\n",
        "w=[ 0.00271756,  0.01260512,  0.00167639, -0.00207756,  0.00720768,\n",
        "   0.00114524,  0.00684168,  0.02242521,  0.01296444]\n",
        "\n",
        "eps=10**-7\n",
        "value= gradient_checking(X[0],y[0],w,eps)\n",
        "grader_grad_check(value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ufbFBFfhgL0"
      },
      "source": [
        "# <font color='red'> Task 2 : Optimizers </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zt05soYh1RM"
      },
      "source": [
        "* As a part of this task, you will be implementing 2  optimizers(methods to update weight)\n",
        "* Use the same computational graph that was mentioned above to do this task\n",
        "* The weights have been initialized from normal distribution with mean=0 and std=0.01. The initialization of weights is very important otherwiswe you can face vanishing gradient and exploding gradients problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAyi7aSAicbr"
      },
      "source": [
        "**Check below video for reference purpose**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogcxiegkQpz"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('gYpoJMlgyXA',width=\"1000\",height=\"500\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmefh7ktjbaR"
      },
      "source": [
        "<font color='blue'><b>Algorithm</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAc4NudkjdNa"
      },
      "source": [
        "<pre>\n",
        "    for each epoch(1-10):\n",
        "        for each data point in your data:\n",
        "            using the functions forward_propagation() and backword_propagation() compute the gradients of weights\n",
        "            update the weigts with help of gradients  \n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fdmPNqtjm3X"
      },
      "source": [
        "## <font color='red'> Implement below tasks</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9-CRsLjx_D"
      },
      "source": [
        "\n",
        "*  <b>Task 2.1</b>: you will be implementing the above algorithm with <b>Vanilla update</b> of weights<br><br>\n",
        "*  <b>Task 2.2</b>: you will be implementing the above algorithm with <b>Momentum update</b> of weights<br><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atp082demrUR"
      },
      "source": [
        "**Note : If you get any assertion error while running grader functions, please print the variables in grader functions and check which variable is returning False .Recheck your logic for that variable .**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv_hZFWalS2z"
      },
      "source": [
        "###<font color='blue'>2.1 Algorithm with Vanilla update of weights</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUZXsJVYwTyX"
      },
      "source": [
        "def vanilla_update(w_vanilla,X,Y,learning_rate,epochs):\n",
        "    mean_loss=[]\n",
        "    for e in range(epochs):\n",
        "        loss_per_datapoint=0\n",
        "        for i in range(len(X)):\n",
        "            #calculate forward propogation\n",
        "            forward_dict = forward_propagation(X[i], y[i], w_vanilla)  \n",
        "            #adding loss for each datapoint\n",
        "            loss_per_datapoint+=forward_dict['loss']    \n",
        "            #calculating gradient dict using backward propogation\n",
        "            gradients = backward_propagation(X[i],y[i],w_vanilla,forward_dict)   \n",
        "            #getting the values of gradients from dictionary\n",
        "            dw = np.array(list(gradients.values()))                                    \n",
        "            \n",
        "            #update w according to vanilla update => w_new = w_old - learning_rate*dw\n",
        "            #you have to write your code here to update weights according to vanilla optimizer\n",
        "            for j in range(len(w_vanilla)):\n",
        "              w_vanilla[j] = w_vanilla[j] - (learning_rate * dw[j])            \n",
        "        # we are appending the average loss for all the datapoints\n",
        "        mean_loss.append(loss_per_datapoint/len(X))\n",
        "    return np.array(mean_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uucCdyQrGhZt"
      },
      "source": [
        "def grader_grad_check(array):\n",
        "    assert(np.round(array[0],3)==0.678 and np.round(array[-1],3)==0.0310)\n",
        "    return True \n",
        "\n",
        "w_vanilla=np.array([ 0.00244274 ,0.00973789,-0.00504212, 0.00146322,-0.00952112, 0.03329146,\n",
        " -0.00042503  ,0.00149911 ,0.0136601 ])\n",
        "learning_rate=0.001\n",
        "epochs=10\n",
        "vanilla_loss_array= vanilla_update(w_vanilla,X,y,learning_rate,epochs)\n",
        "print(grader_grad_check(vanilla_loss_array))\n",
        "plt.plot(np.arange(epochs),vanilla_loss_array)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4RWWrZ7lWap"
      },
      "source": [
        "###<font color='blue'>2.2 Algorithm with Momentum update of weights</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM2BAasL6zz4"
      },
      "source": [
        "<img src='https://i.imgur.com/gyPSXhS.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iCc4sIE-4A-"
      },
      "source": [
        "Here Gamma referes to the momentum coefficient, eta is leaning rate and v_t is moving average of our gradients at timestep t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72rRC8rJGhf5"
      },
      "source": [
        "def momentum_update(w_momentum,X,Y,learning_rate,epochs,v,gamma):\n",
        "    mean_loss=[]\n",
        "    for e in range(epochs):\n",
        "        loss_per_datapoint=0\n",
        "        for i in range(len(X)):\n",
        "            #calculate forward propogation\n",
        "            forward_dict = forward_propagation(X[i], y[i], w_momentum)  \n",
        "            #adding loss for each datapoint\n",
        "            loss_per_datapoint+=forward_dict['loss']    \n",
        "            #calculating gradient dict using backward propogation\n",
        "            gradients = backward_propagation(X[i],y[i],w_momentum,forward_dict)   \n",
        "            #getting the values of gradients from dictionary\n",
        "            dw = np.array(list(gradients.values()))                        \n",
        "\n",
        "            #you have to write your code here to update weights according to momentum optimizer\n",
        "            for j in range(len(w_momentum)):\n",
        "              #change = dw[j] * step_size            \n",
        "              #w_momentum[j] = w_momentum[j] - change\n",
        "\n",
        "              # Momentum update\n",
        "              v[j] = gamma * v[j] - learning_rate * dw[j] # integrate velocity\n",
        "              w_momentum[j] += v[j] # integrate position\n",
        "\n",
        "            \n",
        "        # we are appending the average loss for all the datapoints\n",
        "        mean_loss.append(loss_per_datapoint/len(X))\n",
        "    return np.array(mean_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tutqmPm464xd"
      },
      "source": [
        "def grader_grad_check(array):\n",
        "    assert(np.round(array[0],3)==0.143 and np.round(array[-1],3)==0.005)\n",
        "    return True \n",
        "\n",
        "w_momentum=np.array([ 0.00244274 ,0.00973789,-0.00504212, 0.00146322,-0.00952112, 0.03329146,\n",
        " -0.00042503  ,0.00149911 ,0.0136601 ])\n",
        "\n",
        "learning_rate=0.001\n",
        "epochs=10\n",
        "v=np.zeros(9)                         \n",
        "m=0.9\n",
        "momentum_loss_array= momentum_update(w_momentum,X,y,learning_rate,epochs,v,m)\n",
        "print(grader_grad_check(momentum_loss_array))\n",
        "plt.plot(np.arange(epochs),momentum_loss_array)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7627cy8YlnYO"
      },
      "source": [
        "<font color='blue'>Comparision plot between epochs and loss with different optimizers</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scleSfIXl_bC"
      },
      "source": [
        "\n",
        "plt.plot(vanilla_loss_array, label = 'Vanilla Update')\n",
        "plt.plot(momentum_loss_array, label = 'Momentum Update')\n",
        "\n",
        "\n",
        "plt.title('Comparision b/w epochs & loss with different Optimizers')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfDWEOCr69uQ"
      },
      "source": [
        "<font color='blue'><b>You can go through the following blog to understand the implementation of other optimizers well</font>\n",
        "   <br> [Gradients update blog](https://cs231n.github.io/neural-networks-3/) </br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsoSyrCQ6_xb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}